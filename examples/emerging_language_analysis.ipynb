{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReferentialGym: Example Analysis of the Emerging Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import gc \n",
    "\n",
    "def dict_rec_update(d, u):\n",
    "    if not(isinstance(u,dict)):\n",
    "        #then d and u are lists:\n",
    "        for el in u:\n",
    "            d.append(el)\n",
    "        return d \n",
    "    \n",
    "    for k in u:\n",
    "        if k in d:\n",
    "            d[k] = dict_rec_update(d[k],u[k])\n",
    "        else:\n",
    "            d[k] = u[k]\n",
    "    return d\n",
    "    \n",
    "def check_nbr_epoch_in_data(path):\n",
    "    data = [dict()]\n",
    "    dicts = [(int(fn[10:]), fn) for fn in os.listdir(path) if  'logs.dict.' in fn]\n",
    "    dicts.sort(key=lambda x: x[0])\n",
    "    \n",
    "    d_path = dicts[-1][1]\n",
    "    file = open(os.path.join(path, d_path), \"rb\")\n",
    "    cdata = pickle.load(file)\n",
    "    nbr_epoch = len(cdata)\n",
    "    print(\"There are {} epochs (testing and training).\".format(nbr_epoch))\n",
    "    file.close()\n",
    "    del cdata\n",
    "    gc.collect()\n",
    "    return nbr_epoch\n",
    "    \n",
    "def load_data(path, epoch=0, verbose=True):\n",
    "    dicts = [(int(fn[10:]), fn) for fn in os.listdir(path) if  'logs.dict.' in fn]\n",
    "    dicts.sort(key=lambda x: x[0])\n",
    "    \n",
    "    data = dict()\n",
    "    for didx in tqdm(range(len(dicts))):\n",
    "        d_path = dicts[didx][1]\n",
    "        file = open(os.path.join(path, d_path), \"rb\")\n",
    "        cdata = pickle.load(file)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Looking for epoch {}. Log {} contains {} epochs.\".format(epoch,didx,len(cdata)), end='\\r')\n",
    "        if len(cdata) < epoch+1:\n",
    "            file.close()\n",
    "            gc.collect()\n",
    "            continue      \n",
    "        data = dict_rec_update(data, cdata[epoch])\n",
    "        file.close()\n",
    "        gc.collect()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 epochs (testing and training).\n"
     ]
    }
   ],
   "source": [
    "nbr_epoch = check_nbr_epoch_in_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:05<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "epoch = nbr_epoch-2\n",
    "data = load_data(path, epoch=epoch, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2703"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['os0']['decision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_id in data:\n",
    "    for key in data[agent_id]:\n",
    "        data[agent_id][key] = [ (d[0],d[1]) for d in data[agent_id][key] if d[1] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 proprer decisions for agent ol0.\n",
      "There are 2703 proprer sentences for agent ol0.\n",
      "There are 2703 proprer decisions for agent os0.\n",
      "There are 0 proprer sentences for agent os0.\n"
     ]
    }
   ],
   "source": [
    "for k1 in data:\n",
    "    it_decisions = [ (s[0],s[1]) for s in data[k1]['decision'] if s[1] is not None]\n",
    "    it_sentences = [ (s[0],s[1]) for s in data[k1]['sentences_widx'] if s[1] is not None]\n",
    "    k2 = 'decision'\n",
    "    print(f\"There are {len(it_decisions)} proprer decisions for agent {k1}.\")\n",
    "    k2 = 'sentences_widx'\n",
    "    print(f\"There are {len(it_sentences)} proprer sentences for agent {k1}.\")\n",
    "    #print(f\"Final iteration idx: {data[k1][k2][-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [ values for k,values in data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['decision', 'sentences_widx', 'sentences_logits', 'sentences_one_hot', 'temporal_features'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein Distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.python-course.eu/levenshtein_distance.php\n",
    "def compute_levenshtein_distance(s1, s2):\n",
    "    rows = len(s1)+1\n",
    "    cols = len(s2)+1\n",
    "    dist = [[0 for x in range(cols)] for x in range(rows)]\n",
    "    # source prefixes can be transformed into empty strings \n",
    "    # by deletions:\n",
    "    for i in range(1, rows):\n",
    "        dist[i][0] = i\n",
    "    # target prefixes can be created from an empty source string\n",
    "    # by inserting the characters\n",
    "    for i in range(1, cols):\n",
    "        dist[0][i] = i\n",
    "    \n",
    "    # From there, we can compute iteratively how many steps\n",
    "    # are needed to transform the source prefix (at col) into\n",
    "    # the target prefix (at row):\n",
    "    for i in range(1, rows):\n",
    "        for j in range(1, cols):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            dist[i][j] = min(dist[i-1][j] + 1,      # deletion\n",
    "                                 dist[i][j-1] + 1,      # insertion\n",
    "                                 dist[i-1][j-1] + cost) # substitution\n",
    "    return float(dist[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_levenshtein_distance([0,1,2,3],[0,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]]\n",
      "[[4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(agents[0]['sentences_widx'][4][1])\n",
    "print(agents[0]['sentences_widx'][100][1])\n",
    "compute_levenshtein_distance(agents[0]['sentences_widx'][4][1],agents[0]['sentences_widx'][100][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_sim(v1, v2):\n",
    "    v1_norm = LA.norm(v1)\n",
    "    v2_norm = LA.norm(v2)\n",
    "    cos_sim = np.matmul(v1/v1_norm,(v2/v2_norm).transpose())\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(64,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9801021"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(agents[0]['temporal_features'][4][1].shape)\n",
    "print(agents[0]['temporal_features'][100][1].shape)\n",
    "compute_cosine_sim(agents[0]['temporal_features'][4][1],agents[0]['temporal_features'][100][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Measure of Compositionality as Topographic Similarity (Negative Spearman Correlation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topographic_similarity(sentences,features,comprange=100):\n",
    "    levs = []\n",
    "    for idx1 in tqdm(range(len(sentences))):\n",
    "        s1 = sentences[idx1]\n",
    "        tillidx = min(len(sentences)-1,idx1+1+comprange)\n",
    "        for idx2, s2 in enumerate(sentences[idx1+1:tillidx]): \n",
    "            levs.append( compute_levenshtein_distance(s1,s2))\n",
    "    cossims = []\n",
    "    for idx1 in tqdm(range(len(features))):\n",
    "        f1 = features[idx1]\n",
    "        tillidx = min(len(sentences)-1,idx1+1+comprange)\n",
    "        for idx2, f2 in enumerate(features[idx1+1:tillidx]): \n",
    "            cossims.append( compute_cosine_sim(f1,f2))\n",
    "    \n",
    "    rho, p = spearmanr(levs, cossims)\n",
    "    return -rho, p, levs, cossims  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2703/2703 [13:19<00:00,  3.38it/s]\n",
      "100%|██████████| 2703/2703 [01:16<00:00, 35.24it/s] \n"
     ]
    }
   ],
   "source": [
    "#%pdb on\n",
    "agent_idx = 0\n",
    "nbr_samples = 10000\n",
    "it_sentences = [ (s[0],s[1]) for s in agents[agent_idx]['sentences_widx'][:nbr_samples] if s[1] is not None]\n",
    "sentences = [s[1] for s in it_sentences]\n",
    "features = [] \n",
    "\n",
    "sentence_idx = 0 \n",
    "for idx, (it,tf) in enumerate(agents[agent_idx]['temporal_features']):\n",
    "    if len(features) == len(sentences): \n",
    "            break\n",
    "    \n",
    "    sentence_it = it_sentences[sentence_idx][0]\n",
    "    if sentence_it == it:\n",
    "        features.append(tf)\n",
    "        sentence_idx += 1\n",
    "\n",
    "comprange = 10000\n",
    "rho, p, levs, cossims = compute_topographic_similarity(sentences=sentences, features=features, comprange=comprange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topographic similarity between the language space and the visual feature space: 0.7979867572986369 (with p=0.0).\n"
     ]
    }
   ],
   "source": [
    "print(\"Topographic similarity between the language space and the visual feature space: {} (with p={}).\".format(rho,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0-Levenshtein-distance pairs: 661782 / 3649051 :: 18.13572898816706%.\n",
      "Number of less-than-0.1-cosine-distance pairs: 0 / 3649051 :: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "zero_lev_dist_card = len([l for l in levs if l==0])\n",
    "lev_dist_card = len(levs)\n",
    "print(\"Number of 0-Levenshtein-distance pairs: {} / {} :: {}%.\".format(zero_lev_dist_card, lev_dist_card, float(zero_lev_dist_card)/lev_dist_card*100.0))\n",
    "\n",
    "threshold = 0.1\n",
    "thresholded_cossim_dist_card = len([c for c in cossims if abs(c)< threshold])\n",
    "cossims_dist_card = len(cossims)\n",
    "print(\"Number of less-than-{}-cosine-distance pairs: {} / {} :: {}%.\".format(threshold, thresholded_cossim_dist_card, cossims_dist_card, float(thresholded_cossim_dist_card)/cossims_dist_card*100.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cardinality(data):\n",
    "    if isinstance(data[0], np.ndarray):\n",
    "        data_array = np.concatenate([np.expand_dims(d, 0) for d in data], axis=0)\n",
    "        data_set = np.unique(data_array, axis=0)\n",
    "    else:\n",
    "        data_set = set(data)\n",
    "    return len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary: 4.0 symbols.\n",
      "There are 386 unique sentences out of 2602 different stimuli :: 14.834742505764797 %.\n",
      "Ambiguity: 85.1652574942352 %.\n",
      "The minimum sentence length is: 0. The average sentence length is: 6.256751757306696.\n"
     ]
    }
   ],
   "source": [
    "vocab_size = np.max(np.concatenate([ np.array([symbol for symbol in sentence]) for sentence in sentences]))\n",
    "print(\"The size of the vocabulary: {} symbols.\".format(vocab_size))\n",
    "\n",
    "str_s = [ ''.join([chr(97+int(symbol)) for symbol in sentence if symbol<vocab_size]) for sentence in sentences]\n",
    "nbr_unique_sentences = cardinality(str_s)\n",
    "nbr_unique_stimulus = cardinality(features)\n",
    "\n",
    "print(\"There are {} unique sentences out of {} different stimuli :: {} %.\".format(nbr_unique_sentences, nbr_unique_stimulus, float(nbr_unique_sentences)/nbr_unique_stimulus*100.0))\n",
    "print(\"Ambiguity: {} %.\".format(float(nbr_unique_stimulus-nbr_unique_sentences)/nbr_unique_stimulus*100.0))\n",
    "\n",
    "sentence_lengths = [ len([symbol for symbol in sentence if symbol<vocab_size]) for sentence in sentences]\n",
    "\n",
    "'''\n",
    "fullstr_s = [ ''.join([chr(97+int(symbol)) for symbol in sentence]) for sentence in sentences]\n",
    "fullsentence_lengths = [ (len(sentence.replace('f','')), sentence) for sentence in fullstr_s]\n",
    "for i in range(1000):\n",
    "    print(fullsentence_lengths[i:i+1])\n",
    "'''\n",
    "\n",
    "min_s_length = min(sentence_lengths)\n",
    "mean_s_length = np.mean(np.array(sentence_lengths))\n",
    "print(\"The minimum sentence length is: {}. The average sentence length is: {}.\".format(min_s_length, mean_s_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguity-regularized Compositionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguity-regularized topographic similarity: 0.11837928067535505 (with p=0.0).\n"
     ]
    }
   ],
   "source": [
    "amb = float(nbr_unique_stimulus-nbr_unique_sentences)/nbr_unique_stimulus\n",
    "print(\"Ambiguity-regularized topographic similarity: {} (with p={}).\".format(rho*(1-amb),p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compositionality of unique sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/632 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 632 unique sentences out of the 2703 sampled sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 632/632 [00:45<00:00, 13.92it/s]\n",
      "100%|██████████| 632/632 [00:03<00:00, 183.74it/s]\n"
     ]
    }
   ],
   "source": [
    "#%pdb on\n",
    "agent_idx = 0\n",
    "nbr_samples = 20000\n",
    "\n",
    "it_sentences = [ (s[0],s[1]) for s in agents[agent_idx]['sentences_widx'][:nbr_samples] if s[1] is not None]\n",
    "\n",
    "sentences = [s[1] for s in it_sentences]\n",
    "np_sentences = np.concatenate( sentences, axis=1).transpose(1,0)\n",
    "\n",
    "_, idx_unique_sentences = np.unique(np_sentences, axis=0, return_index=True)\n",
    "idx_unique_sentences = sorted(idx_unique_sentences)\n",
    "\n",
    "unique_it_sentences = [ it_sentences[idx] for idx in idx_unique_sentences]\n",
    "unique_sentences = [s[1] for s in unique_it_sentences]\n",
    "\n",
    "features = [ tf for idx, (it, tf) in enumerate(agents[agent_idx]['temporal_features'][:nbr_samples]) if idx in idx_unique_sentences]\n",
    "\n",
    "print(\"There are {} unique sentences out of the {} sampled sentences.\".format(len(unique_sentences), len(it_sentences)))\n",
    "\n",
    "comprange = 20000\n",
    "rho, p, levs, cossims = compute_topographic_similarity(sentences=unique_sentences, features=features, comprange=comprange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguity-agnostic Topographic similarity: 0.8186521349080202 (with p=0.0).\n"
     ]
    }
   ],
   "source": [
    "print(\"Ambiguity-agnostic Topographic similarity: {} (with p={}).\".format(rho,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
